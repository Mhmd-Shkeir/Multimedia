{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74be3993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MAHDI\\anaconda3\\envs\\sneaker\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import pickle\n",
    "from typing import Dict, Tuple, List\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7fed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAHDI\\AppData\\Local\\Temp\\ipykernel_28452\\290939741.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 model loaded.\n",
      "Predicted class: nike_air_jordan_1_low\n",
      "Confidence: 0.976\n",
      "→ This image is a valid sneaker match.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('nike_air_jordan_1_low', 0.9757837653160095)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===== CONFIG =====\n",
    "MODEL_PATH = \"best_resnet50_sneakers.pt\"\n",
    "CLASS_INDICES_PATH = \"class_indices.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 50\n",
    "CONFIDENCE_THRESHOLD = 0.20\n",
    "\n",
    "if Path(CLASS_INDICES_PATH).exists():\n",
    "    with open(CLASS_INDICES_PATH, \"r\") as f:\n",
    "        raw_map = json.load(f)\n",
    "\n",
    "    # raw_map is:  {\"adidas_forum_high\": 0, \"nike_air_force_1\": 1, ...}\n",
    "    class_map = {v: k for k, v in raw_map.items()}\n",
    "else:\n",
    "    raise FileNotFoundError(\"class_indices.json not found.\")\n",
    "\n",
    "\n",
    "\n",
    "classifier_model = models.resnet50(weights=None)\n",
    "classifier_model.fc = nn.Sequential(\n",
    "    nn.Linear(classifier_model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "classifier_model.load_state_dict(state, strict=True)\n",
    "classifier_model.eval().to(DEVICE)\n",
    "\n",
    "print(\"ResNet50 model loaded.\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def predict_sneaker_class(img_path, threshold=CONFIDENCE_THRESHOLD):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = classifier_model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "\n",
    "    top_idx = int(torch.argmax(probs))\n",
    "    conf = float(probs[top_idx])\n",
    "    class_name = class_map[top_idx]\n",
    "\n",
    "    print(f\"Predicted class: {class_name}\")\n",
    "    print(f\"Confidence: {conf:.3f}\")\n",
    "\n",
    "    if conf < threshold:\n",
    "        print(\"→ This image is probably NOT a known sneaker (below threshold).\")\n",
    "    else:\n",
    "        print(\"→ This image is a valid sneaker match.\")\n",
    "\n",
    "    return class_name, conf\n",
    "\n",
    "predict_sneaker_class(\"testing images/jordan1 low.jpeg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351c6bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached index for nike_air_jordan_1_low...\n",
      "Using query-time augmentation...\n",
      "\n",
      "Top 5 matches in nike_air_jordan_1_low:\n",
      " 1. air-jordan-1-low-alt-ps-cobalt-bliss-fn7376-400 | FN7376_400.png.png (sim: 0.850)\n",
      " 2. air-jordan-1-low-black-white-dark-gum-hv5968-001 | 1526203_02.jpg.jpeg (sim: 0.849)\n",
      " 3. air-jordan-1-low-spruce-aura-cw1381-003 | 601728_08.jpg.jpeg (sim: 0.847)\n",
      " 4. air-jordan-1-low-iron-grey-553558-152 | 1452846_02.jpg.jpeg (sim: 0.846)\n",
      " 5. air-jordan-1-low-alt-td-black-medium-olive-dr9747-092 | DR9747_092.png.png (sim: 0.845)\n"
     ]
    }
   ],
   "source": [
    "# image_search_class_scoped_v2.py\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_ROOT = Path(\"Scraping_part/goat_data\")\n",
    "INDEX_CACHE_DIR = Path(\"faiss_cache\")\n",
    "INDEX_CACHE_DIR.mkdir(exist_ok=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==== MODEL ====\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "# Cache for indices\n",
    "_index_cache: Dict[str, Tuple[faiss.Index, List[str]]] = {}\n",
    "\n",
    "# ==== AUGMENTATION ====\n",
    "def augment_image(img: Image.Image, strength='light'):\n",
    "    \"\"\"Apply random augmentations to image\"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    # Original\n",
    "    augmented.append(img)\n",
    "    \n",
    "    if strength in ['light', 'medium', 'heavy']:\n",
    "        # Brightness variations\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        augmented.append(enhancer.enhance(random.uniform(0.85, 1.15)))\n",
    "        \n",
    "        # Contrast variations\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        augmented.append(enhancer.enhance(random.uniform(0.9, 1.1)))\n",
    "    \n",
    "    if strength in ['medium', 'heavy']:\n",
    "        # Color saturation\n",
    "        enhancer = ImageEnhance.Color(img)\n",
    "        augmented.append(enhancer.enhance(random.uniform(0.9, 1.1)))\n",
    "        \n",
    "        # Slight rotation\n",
    "        augmented.append(img.rotate(random.uniform(-10, 10), fillcolor=(255, 255, 255)))\n",
    "        \n",
    "        # Zoom crop (simulates different distances)\n",
    "        w, h = img.size\n",
    "        crop_size = int(min(w, h) * random.uniform(0.85, 0.95))\n",
    "        left = random.randint(0, w - crop_size)\n",
    "        top = random.randint(0, h - crop_size)\n",
    "        cropped = img.crop((left, top, left + crop_size, top + crop_size))\n",
    "        augmented.append(cropped.resize((w, h), Image.LANCZOS))\n",
    "    \n",
    "    if strength == 'heavy':\n",
    "        # Horizontal flip (only if makes sense for your use case)\n",
    "        augmented.append(img.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "        \n",
    "        # Sharpness\n",
    "        enhancer = ImageEnhance.Sharpness(img)\n",
    "        augmented.append(enhancer.enhance(random.uniform(0.8, 1.2)))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "def embed_image(path: Path, augment=False, aug_strength='light'):\n",
    "    \"\"\"Embed image with optional augmentation\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "    if not augment:\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.get_image_features(**inputs)\n",
    "        v = emb[0].cpu().numpy()\n",
    "        return v / np.linalg.norm(v)\n",
    "    \n",
    "    # Multi-augmentation embedding\n",
    "    imgs = augment_image(img, strength=aug_strength)\n",
    "    embeddings = []\n",
    "    \n",
    "    for aug_img in imgs:\n",
    "        inputs = processor(images=aug_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.get_image_features(**inputs)\n",
    "        v = emb[0].cpu().numpy()\n",
    "        embeddings.append(v / np.linalg.norm(v))\n",
    "    \n",
    "    # Return mean of augmented embeddings\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def build_class_index(class_dir: Path, augment_index=False, aug_per_image=5):\n",
    "    \"\"\"\n",
    "    Build FAISS index for a class. Optionally augment each image multiple times.\n",
    "    \n",
    "    Args:\n",
    "        class_dir: Path to class directory\n",
    "        augment_index: If True, create multiple augmented versions per image\n",
    "        aug_per_image: Number of augmented versions per original image\n",
    "    \"\"\"\n",
    "    vectors, paths = [], []\n",
    "\n",
    "    for slug_dir in class_dir.iterdir():\n",
    "        if not slug_dir.is_dir():\n",
    "            continue\n",
    "        for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "            for img_path in slug_dir.glob(ext):\n",
    "                try:\n",
    "                    if not augment_index:\n",
    "                        v = embed_image(img_path)\n",
    "                        vectors.append(v.astype(\"float32\"))\n",
    "                        paths.append(str(img_path))\n",
    "                    else:\n",
    "                        # Create multiple augmented embeddings per image\n",
    "                        img = Image.open(img_path).convert(\"RGB\")\n",
    "                        augmented_imgs = augment_image(img, strength='medium')\n",
    "                        \n",
    "                        # Limit augmentations\n",
    "                        for aug_img in augmented_imgs[:aug_per_image]:\n",
    "                            inputs = processor(images=aug_img, return_tensors=\"pt\").to(DEVICE)\n",
    "                            with torch.no_grad():\n",
    "                                emb = model.get_image_features(**inputs)\n",
    "                            v = emb[0].cpu().numpy()\n",
    "                            v_norm = v / np.linalg.norm(v)\n",
    "                            vectors.append(v_norm.astype(\"float32\"))\n",
    "                            paths.append(str(img_path))  # Same path for all augmentations\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Skip {img_path}: {e}\")\n",
    "\n",
    "    if not vectors:\n",
    "        raise RuntimeError(f\"No images found under {class_dir.resolve()}\")\n",
    "\n",
    "    arr = np.stack(vectors)\n",
    "    faiss.normalize_L2(arr)\n",
    "    \n",
    "    # Use IndexFlatIP for exact cosine similarity\n",
    "    index = faiss.IndexFlatIP(arr.shape[1])\n",
    "    index.add(arr)\n",
    "    \n",
    "    unique_images = len(set(paths))\n",
    "    print(f\"Indexed {len(paths)} embeddings ({unique_images} unique images) for {class_dir.name}\")\n",
    "    return index, paths\n",
    "\n",
    "def get_or_build_index(class_name: str, rebuild=False, augment_index=False):\n",
    "    \"\"\"Get cached index or build new one\"\"\"\n",
    "    if class_name in _index_cache and not rebuild:\n",
    "        return _index_cache[class_name]\n",
    "    \n",
    "    cache_file = INDEX_CACHE_DIR / f\"{class_name}.pkl\"\n",
    "    \n",
    "    # Try loading from disk cache\n",
    "    if cache_file.exists() and not rebuild:\n",
    "        print(f\"Loading cached index for {class_name}...\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        _index_cache[class_name] = (\n",
    "            faiss.deserialize_index(index_data['index']),\n",
    "            index_data['paths']\n",
    "        )\n",
    "        return _index_cache[class_name]\n",
    "    \n",
    "    # Build new index\n",
    "    class_dir = DATA_ROOT / class_name\n",
    "    index, paths = build_class_index(class_dir, augment_index=augment_index)\n",
    "    \n",
    "    # Cache to disk\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'index': faiss.serialize_index(index),\n",
    "            'paths': paths\n",
    "        }, f)\n",
    "    \n",
    "    _index_cache[class_name] = (index, paths)\n",
    "    return index, paths\n",
    "\n",
    "def search_in_class(query_img, class_name, top_k=5, use_query_augmentation=True, \n",
    "                   augment_index=False, rebuild_index=False):\n",
    "    \"\"\"\n",
    "    Search within one class with improved accuracy.\n",
    "    \n",
    "    Args:\n",
    "        query_img: Path to query image\n",
    "        class_name: Class directory name\n",
    "        top_k: Number of results to return\n",
    "        use_query_augmentation: Average multiple augmented query embeddings\n",
    "        augment_index: Build index with augmented images (slower, better recall)\n",
    "        rebuild_index: Force rebuild cached index\n",
    "    \"\"\"\n",
    "    # Get or build index\n",
    "    index, paths = get_or_build_index(class_name, rebuild=rebuild_index, \n",
    "                                     augment_index=augment_index)\n",
    "    \n",
    "    # Embed query with optional augmentation\n",
    "    if use_query_augmentation:\n",
    "        print(\"Using query-time augmentation...\")\n",
    "        qvec = embed_image(Path(query_img), augment=True, aug_strength='medium')\n",
    "    else:\n",
    "        qvec = embed_image(Path(query_img))\n",
    "    \n",
    "    qvec = qvec.astype(\"float32\").reshape(1, -1)\n",
    "    faiss.normalize_L2(qvec)\n",
    "    \n",
    "    # Search with more candidates for deduplication\n",
    "    search_k = top_k * 10 if augment_index else top_k\n",
    "    sims, idxs = index.search(qvec, search_k)\n",
    "    \n",
    "    # Deduplicate results (same path might appear multiple times due to augmentation)\n",
    "    seen_paths = {}\n",
    "    for i, score in zip(idxs[0], sims[0]):\n",
    "        path = paths[i]\n",
    "        if path not in seen_paths or score > seen_paths[path]:\n",
    "            seen_paths[path] = score\n",
    "    \n",
    "    # Sort and take top_k unique results\n",
    "    results = sorted(seen_paths.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    print(f\"\\nTop {top_k} matches in {class_name}:\")\n",
    "    for rank, (path, score) in enumerate(results, start=1):\n",
    "        # Extract model name from path\n",
    "        model_name = Path(path).parent.name\n",
    "        print(f\"{rank:>2}. {model_name} | {Path(path).name} (sim: {score:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==== USAGE EXAMPLES ====\n",
    "if __name__ == \"__main__\":\n",
    "    #use model to predict image class\n",
    "    \n",
    "    # Example 1: Basic search with query augmentation (RECOMMENDED)\n",
    "    search_in_class(\n",
    "        query_img=\"testing images/jordan 1 low 2).jpeg\",\n",
    "        class_name=\"nike_air_jordan_1_low\",\n",
    "        top_k=5,\n",
    "        use_query_augmentation=True  # Fast, significant accuracy boost\n",
    "    )\n",
    "    \n",
    "    # Example 2: Build augmented index (slower but better for small datasets)\n",
    "    # Run this once to rebuild your indices with augmentation\n",
    "    # search_in_class(\n",
    "    #     query_img=\"path/to/query.jpg\",\n",
    "    #     class_name=\"\",\n",
    "    #     top_k=5,\n",
    "    #     use_query_augmentation=True,\n",
    "    #     augment_index=True,  # 5x more embeddings in index\n",
    "    #     rebuild_index=True   # Force rebuild\n",
    "    # )\n",
    "    \n",
    "    # Example 3: Rebuild all class indices with augmentation\n",
    "    # for class_dir in DATA_ROOT.iterdir():\n",
    "    #     if class_dir.is_dir():\n",
    "    #         get_or_build_index(class_dir.name, rebuild=True, augment_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ed851d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: nike_air_jordan_1_low\n",
      "Confidence: 0.976\n",
      "→ This image is a valid sneaker match.\n",
      "\n",
      "Searching inside predicted class: nike_air_jordan_1_low\n",
      "\n",
      "Using query-time augmentation...\n",
      "\n",
      "Top 5 matches in nike_air_jordan_1_low:\n",
      " 1. wmns-air-jordan-1-low-french-blue-dc0774-402 | 1274656_08.jpg.jpeg (sim: 0.856)\n",
      " 2. air-jordan-1-low-golf-midnight-navy-dd9315-104 | 1114577_03.jpg.jpeg (sim: 0.856)\n",
      " 3. air-jordan-1-low-gs-usa-cv9844-400 | 672670_08.jpg.jpeg (sim: 0.853)\n",
      " 4. wmns-air-jordan-1-low-marina-blue-dc0774-114 | 913992_08.jpg.jpeg (sim: 0.850)\n",
      " 5. air-jordan-1-low-gs-game-royal-553560-124 | 664458_08.jpg.jpeg (sim: 0.847)\n"
     ]
    }
   ],
   "source": [
    "#example full usage:\n",
    "# ===== BLOCK 3: Example end-to-end usage =====\n",
    "\n",
    "QUERY_IMAGE = \"testing images/jordan1 low.jpeg\"   # change as needed\n",
    "\n",
    "# 1) Predict sneaker class using ResNet50\n",
    "predicted_class, confidence = predict_sneaker_class(QUERY_IMAGE)\n",
    "\n",
    "# 2) Only search if confidence is above your threshold\n",
    "if confidence >= CONFIDENCE_THRESHOLD:\n",
    "    print(f\"\\nSearching inside predicted class: {predicted_class}\\n\")\n",
    "    \n",
    "    # 3) Run FAISS search (uses your full implementation)\n",
    "    results = search_in_class(\n",
    "        query_img=QUERY_IMAGE,\n",
    "        class_name=predicted_class,\n",
    "        top_k=5,\n",
    "        use_query_augmentation=True\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nConfidence too low — skipping FAISS similarity search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0f03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Resale Price: $91.93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ===============================\n",
    "# Load trained model\n",
    "# ===============================\n",
    "model_path = \"sneaker_price_model.cbm\"\n",
    "\n",
    "model = CatBoostRegressor()\n",
    "model.load_model(model_path)\n",
    "\n",
    "# ===============================\n",
    "# Example input\n",
    "# Replace these values with real inputs\n",
    "# ===============================\n",
    "input_data = {\n",
    "    \"class_name\": \"adidas-forum-high\",\n",
    "    \"brand\": \"adidas\",\n",
    "    \"silhouette\": \"forum\",\n",
    "    \"retail_price_usd\": 230, # jeeba min l excel ba3d ma tle2e l sneaker wa 3mella input\n",
    "    \"release_age\": 3 # jeeba min l excel ba3d ma tle2e l sneaker aw 3mella input\n",
    "}\n",
    "\n",
    "# Convert to DataFrame (CatBoost expects tabular format)\n",
    "df_input = pd.DataFrame([input_data])\n",
    "\n",
    "# ===============================\n",
    "# Predict\n",
    "# ===============================\n",
    "predicted_price = model.predict(df_input)[0]\n",
    "\n",
    "print(f\"Predicted Resale Price: ${predicted_price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f31dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'adidas-forum-high', 'brand': 'adidas', 'silhouette': 'Forum', 'retail_price_usd': 120.0, 'release_age': 3}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV once\n",
    "df = pd.read_csv(\"Scraping_part/scraper/data/products_nodup.csv\")\n",
    "df[\"slug\"] = df[\"slug\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# release_age\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
    "df[\"release_age\"] = 2025 - df[\"release_date\"].dt.year\n",
    "\n",
    "def get_input_data(slug: str):\n",
    "    slug = slug.lower().strip()\n",
    "    row = df[df[\"slug\"] == slug]\n",
    "\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Slug not found: {slug}\")\n",
    "\n",
    "    r = row.iloc[0]\n",
    "\n",
    "    return {\n",
    "        \"class_name\": r[\"class_name\"],\n",
    "        \"brand\": r[\"brand\"],\n",
    "        \"silhouette\": r[\"silhouette\"],\n",
    "        \"retail_price_usd\": float(r[\"retail_price_usd\"]),\n",
    "        \"release_age\": int(r[\"release_age\"]) if pd.notna(r[\"release_age\"]) else None,\n",
    "    }\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    print(get_input_data(\"forum-84-high-white-black-gy5847\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15035d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sneaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
