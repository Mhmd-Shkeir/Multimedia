{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93413d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAHDI\\AppData\\Local\\Temp\\ipykernel_29556\\305872777.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(MODEL_PATH, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# image_search_test.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_ROOT = Path(\"Scraping_part\\goat_data\")  # each subfolder = class, contains slugs/images\n",
    "MODEL_PATH = \"best_resnet50_sneakers.pt\"\n",
    "\n",
    "# ==== MODEL ====\n",
    "NUM_CLASSES = 50\n",
    "base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "base.fc = nn.Sequential(\n",
    "    nn.Linear(base.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "state = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "base.load_state_dict(state, strict=True)\n",
    "feature_extractor = nn.Sequential(*list(base.children())[:-1]).eval()  # 2048-d after avgpool\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "feature_extractor.to(device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def embed_image(p: Path) -> np.ndarray:\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    x = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        v = feature_extractor(x).squeeze().cpu().numpy()  # [2048]\n",
    "    v = v / (np.linalg.norm(v) + 1e-8)\n",
    "    return v.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fe07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a546a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.769\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== LOAD MODEL ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "# ==== EMBEDDING FUNCTION ====\n",
    "def embed_image(path: str):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "    vec = emb[0].cpu().numpy()\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "# ==== TEST ====\n",
    "img1 = Path(\"s-l1200.jpg\")\n",
    "img2 = Path(r\"Scraping_part\\goat_data\\adidas_forum_high\\cuts-slices-x-forum-84-high-pull-up-beloved-fz6567\\1045923_01.jpg.jpeg\")\n",
    "\n",
    "\n",
    "v1 = embed_image(img1)\n",
    "v2 = embed_image(img2)\n",
    "\n",
    "similarity = np.dot(v1, v2)  # cosine similarity since both normalized\n",
    "print(f\"Similarity: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719a41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adidas_forum_high: indexed 30 images.\n",
      "adidas_forum_low: indexed 20 images.\n",
      "adidas_gazelle: indexed 56 images.\n",
      "adidas_nmd_r1: indexed 38 images.\n",
      "adidas_samba: indexed 46 images.\n",
      "adidas_stan_smith: indexed 79 images.\n",
      "adidas_superstar: indexed 88 images.\n",
      "adidas_ultraboost: indexed 59 images.\n",
      "asics_gel-lyte_iii: indexed 85 images.\n",
      "converse_chuck_70_high: indexed 77 images.\n",
      "converse_chuck_70_low: indexed 70 images.\n",
      "converse_chuck_taylor_all-star_high: indexed 83 images.\n",
      "converse_chuck_taylor_all-star_low: indexed 102 images.\n",
      "converse_one_star: indexed 76 images.\n",
      "new_balance_327: indexed 48 images.\n",
      "new_balance_550: indexed 31 images.\n",
      "new_balance_574: indexed 61 images.\n",
      "new_balance_992: indexed 18 images.\n",
      "nike_air_force_1_high: indexed 76 images.\n",
      "nike_air_force_1_low: indexed 44 images.\n",
      "nike_air_force_1_mid: indexed 85 images.\n",
      "nike_air_jordan_11: indexed 56 images.\n",
      "nike_air_jordan_1_high: indexed 47 images.\n",
      "nike_air_jordan_1_low: indexed 44 images.\n",
      "nike_air_jordan_3: indexed 37 images.\n",
      "nike_air_jordan_4: indexed 45 images.\n",
      "nike_air_max_1: indexed 66 images.\n",
      "nike_air_max_270: indexed 49 images.\n",
      "nike_air_max_90: indexed 54 images.\n",
      "nike_air_max_95: indexed 72 images.\n",
      "nike_air_max_97: indexed 41 images.\n",
      "nike_air_max_plus_(tn): indexed 55 images.\n",
      "nike_air_vapormax_flyknit: indexed 28 images.\n",
      "nike_air_vapormax_plus: indexed 76 images.\n",
      "nike_blazer_mid_77: indexed 32 images.\n",
      "nike_cortez: indexed 63 images.\n",
      "nike_dunk_high: indexed 58 images.\n",
      "nike_dunk_low: indexed 43 images.\n",
      "puma_suede_classic: indexed 78 images.\n",
      "reebok_classic_leather: indexed 70 images.\n",
      "reebok_club_c_85: indexed 73 images.\n",
      "salomon_xt-6: indexed 52 images.\n",
      "Built 42 class indexes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- build: one index per class ---\n",
    "class2index = {}   # class -> faiss index\n",
    "class2paths = {}   # class -> [image paths]\n",
    "\n",
    "for class_dir in sorted(p for p in DATA_ROOT.iterdir() if p.is_dir()):\n",
    "    cls = class_dir.name\n",
    "    vecs, paths = [], []\n",
    "\n",
    "    for slug_dir in (d for d in class_dir.iterdir() if d.is_dir()):\n",
    "        for img_path in slug_dir.glob(\"*.[jp][pn]g\"):\n",
    "            vecs.append(embed_image(img_path))\n",
    "            paths.append(str(img_path))\n",
    "\n",
    "    if not vecs:\n",
    "        continue\n",
    "    V = np.stack(vecs)\n",
    "    faiss.normalize_L2(V)\n",
    "    idx = faiss.IndexFlatIP(V.shape[1])   # cosine via dot product\n",
    "    idx.add(V)\n",
    "\n",
    "    class2index[cls] = idx\n",
    "    class2paths[cls]  = paths\n",
    "    print(f\"{cls}: indexed {len(paths)} images.\")\n",
    "\n",
    "print(f\"Built {len(class2index)} class indexes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a8926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Scraping_part\\goat_data\\adidas_forum_high\\forum-84-high-off-white-team-dark-green-gw4328\\GW4328.png.png  score=0.765\n",
      "2. Scraping_part\\goat_data\\adidas_forum_high\\girls-are-awesome-x-wmns-forum-high-cloud-white-purple-gy2632\\GY2632.png.png  score=0.762\n",
      "3. Scraping_part\\goat_data\\adidas_forum_high\\packer-shoes-x-forum-84-high-college-pack-collegiate-maroon-gx1520\\GX1520.png.png  score=0.758\n",
      "4. Scraping_part\\goat_data\\adidas_forum_high\\snipes-x-forum-high-313-day-gz8376\\GZ8376.png.png  score=0.749\n",
      "5. Scraping_part\\goat_data\\adidas_forum_high\\packer-shoes-x-forum-84-high-college-pack-collegiate-green-gx1519\\GX1519.png.png  score=0.746\n"
     ]
    }
   ],
   "source": [
    "def search_in_class(cls: str, query_img: str, k=5):\n",
    "    if cls not in class2index:\n",
    "        raise ValueError(f\"class '{cls}' not found\")\n",
    "    q = embed_image(Path(query_img)).reshape(1,-1)\n",
    "    faiss.normalize_L2(q)\n",
    "    D, I = class2index[cls].search(q, k)\n",
    "    out = [(class2paths[cls][int(i)], float(s)) for i, s in zip(I[0], D[0])]\n",
    "    return out\n",
    "\n",
    "# --- example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # choose the class you want to restrict to\n",
    "    target_class = \"adidas_forum_high\"\n",
    "    query_path = \"s-l1200.jpg\"  # your test image\n",
    "\n",
    "    results = search_in_class(target_class, query_path, k=5)\n",
    "    for r, (p, score) in enumerate(results, 1):\n",
    "        print(f\"{r}. {p}  score={score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ccd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_search_class_scoped.py\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_ROOT = Path(\"Scraping_part/goat_data\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==== MODEL ====\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "# ==== FUNCTIONS ====\n",
    "def embed_image(path: Path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "    v = emb[0].cpu().numpy()\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "def build_class_index(class_dir: Path):\n",
    "    \"\"\"Embed every image under goat_data/<class>/<slug> folders and build a FAISS index.\"\"\"\n",
    "    vectors, paths = [], []\n",
    "\n",
    "    for slug_dir in class_dir.iterdir():\n",
    "        if not slug_dir.is_dir():\n",
    "            continue\n",
    "        for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "            for img_path in slug_dir.glob(ext):\n",
    "                try:\n",
    "                    v = embed_image(img_path)\n",
    "                    vectors.append(v.astype(\"float32\"))\n",
    "                    paths.append(str(img_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"Skip {img_path}: {e}\")\n",
    "\n",
    "    if not vectors:\n",
    "        raise RuntimeError(f\"No images found under {class_dir.resolve()}\")\n",
    "\n",
    "    arr = np.stack(vectors)\n",
    "    faiss.normalize_L2(arr)\n",
    "    index = faiss.IndexFlatIP(arr.shape[1])\n",
    "    index.add(arr)\n",
    "    print(f\"Indexed {len(paths)} images for {class_dir.name}\")\n",
    "    return index, paths\n",
    "\n",
    "\n",
    "def search_in_class(query_img, class_name, top_k=5):\n",
    "    \"\"\"Search only within one clean_class folder.\"\"\"\n",
    "    class_dir = DATA_ROOT / class_name\n",
    "    index, paths = build_class_index(class_dir)\n",
    "    qvec = embed_image(Path(query_img)).astype(\"float32\").reshape(1, -1)\n",
    "    faiss.normalize_L2(qvec)\n",
    "    sims, idxs = index.search(qvec, top_k)\n",
    "    print(f\"\\nTop {top_k} matches in {class_name}:\")\n",
    "    for rank, (i, score) in enumerate(zip(idxs[0], sims[0]), start=1):\n",
    "        print(f\"{rank:>2}. {paths[i]} (similarity {score:.3f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e59a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 270 images for converse_one_star\n",
      "\n",
      "Top 5 matches in converse_one_star:\n",
      " 1. Scraping_part\\goat_data\\converse_one_star\\one-star-low-triple-black-162950c\\162950C.png.png (similarity 0.765)\n",
      " 2. Scraping_part\\goat_data\\converse_one_star\\awake-ny-x-one-star-pro-low-black-a07143c\\A07143C.png.png (similarity 0.763)\n",
      " 3. Scraping_part\\goat_data\\converse_one_star\\undefeated-x-one-star-academy-pro-black-a12131c\\1505868_03.jpg.jpeg (similarity 0.755)\n",
      " 4. Scraping_part\\goat_data\\converse_one_star\\golf-le-fleur-x-one-star-suede-mono-black-162129c\\356438_08.jpg.jpeg (similarity 0.754)\n",
      " 5. Scraping_part\\goat_data\\converse_one_star\\hello-kitty-x-one-star-low-top-velcro-td-black-763908c\\763908C.png.png (similarity 0.754)\n"
     ]
    }
   ],
   "source": [
    "# ==== RUN EXAMPLE ====\n",
    "query = \"testing images/images (11).jpeg\"  # any sneaker photo\n",
    "search_in_class(query, \"converse_one_star\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6126d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sneaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
